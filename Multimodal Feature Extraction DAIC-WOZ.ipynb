{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567f9f3e-6a02-46a7-892f-be7fb4a27428",
   "metadata": {},
   "source": [
    "## Project Directory and File Structure Inspection\n",
    "Displays the current working directory contents to verify available files, datasets, and paths before loading data or running experiments, helping prevent file-not-found and path-related errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f24bed67-ebcd-411c-a787-e464a3557ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is 6A5C-4620\n",
      "\n",
      " Directory of D:\\portfolio\\IU\\Thesis_Final\\Coding\n",
      "\n",
      "01/06/2026  02:26 PM    <DIR>          .\n",
      "01/05/2026  03:28 PM    <DIR>          ..\n",
      "01/05/2026  10:00 PM    <DIR>          .ipynb_checkpoints\n",
      "01/05/2026  03:28 PM        29,200,044 492_AUDIO.wav\n",
      "01/05/2026  03:28 PM         3,171,528 492_CLNF_AUs.txt\n",
      "01/05/2026  03:28 PM        33,849,177 492_CLNF_features.txt\n",
      "01/05/2026  03:28 PM        50,404,127 492_CLNF_features3D.txt\n",
      "01/05/2026  03:28 PM         4,252,001 492_CLNF_gaze.txt\n",
      "01/05/2026  03:28 PM       489,246,000 492_CLNF_hog.bin\n",
      "01/05/2026  03:28 PM         2,375,549 492_CLNF_pose.txt\n",
      "01/05/2026  03:28 PM        51,383,596 492_COVAREP.csv\n",
      "01/05/2026  03:28 PM         3,017,567 492_FORMANT.csv\n",
      "01/05/2026  03:28 PM            17,295 492_TRANSCRIPT.csv\n",
      "01/06/2026  12:12 AM           228,413 DAIC_WOZ Dataset.ipynb\n",
      "01/05/2026  03:42 PM    <DIR>          DAIC_WOZ_subset\n",
      "01/06/2026  01:07 PM           183,954 master_dataset_.csv\n",
      "01/06/2026  01:56 PM         1,091,598 master_dataset__.csv\n",
      "01/06/2026  02:26 PM            79,929 Thesis Restart.ipynb\n",
      "              14 File(s)    668,500,778 bytes\n",
      "               4 Dir(s)  53,016,543,232 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c13b31-aa9a-416a-9cc8-6af205637a6c",
   "metadata": {},
   "source": [
    "### Library Imports, Global Configuration, and Reproducibility Setup\n",
    "Imports all required libraries for file handling, data processing, and feature engineering, while setting a fixed random seed and suppressing warnings to ensure consistent, clean, and reproducible experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c89cca3e-e057-48b1-ad7a-5ddad2a442ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f09fb-19e0-466a-aa4e-94b6b3eecd5e",
   "metadata": {},
   "source": [
    "### Dataset Paths and Directory Configuration\n",
    "Defines the base project directory and centralizes file paths for extracted participant data and official train/test split files, ensuring consistent, organized, and portable access to all dataset resources throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a2d62546-2fc0-4d7c-b10b-cd2ffb96cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path(\"E:/IU\")\n",
    "\n",
    "EXTRACT_DIR = BASE_PATH / \"DAIC_WOZ_subset\" / \"extracted\"\n",
    "\n",
    "TRAIN_SPLIT_PATH = BASE_PATH / \"DAIC_WOZ_subset\" / \"train_split_Depression_AVEC2017.csv\"\n",
    "\n",
    "TEST_PATH = BASE_PATH / \"DAIC_WOZ_subset\" / \"train_split_Depression_AVEC2017.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40beda8-8e63-429f-ae27-3711179726ce",
   "metadata": {},
   "source": [
    "### Multimodal Feature Extraction from Raw Behavioral Signals\n",
    "Defines a complete set of preprocessing functions that convert raw DAIC-WOZ modality files (transcripts, facial action units, eye gaze, audio, head pose, facial geometry, and HOG descriptors) into clean, aggregated numerical features. Each function summarizes frame-level signals into stable statistics (means/stds or text) to create participant-level representations suitable for machine learning and multimodal fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c504b8b4-c7b0-48d5-a060-c631bbd987c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. The Words: Linguistic Clues\n",
    "def process_words(transcript_path):\n",
    "    df = pd.read_csv(transcript_path, sep='\\t').fillna(\"\")\n",
    "    # Only keep what the Patient said\n",
    "    participant_speech = df[df['speaker'] == 'Participant']['value'].tolist()\n",
    "    # Join all sentences into one long story\n",
    "    full_text = \" \".join(participant_speech)\n",
    "    return full_text\n",
    "\n",
    "## 2. The Face: (Expression Clues)\n",
    "def process_face_aus(au_path):\n",
    "    df_au = pd.read_csv(au_path, sep=',', engine='python')\n",
    "    df_au.columns = df_au.columns.str.strip()\n",
    "    # Get the average \"strength\" of every facial muscle movement\n",
    "    avg_aus = df_au.filter(regex='_r$').mean().to_dict()\n",
    "    return avg_aus\n",
    "\n",
    "## 3. The Eyes (Behavioral Clues)\n",
    "def process_eyes_carefully(gaze_path):\n",
    "    # Load the file\n",
    "    df_gaze = pd.read_csv(gaze_path, sep=None)\n",
    "    df_gaze.columns = df_gaze.columns.str.strip()\n",
    "\n",
    "    # FILTER: Only keep rows where the computer actually saw the eyes\n",
    "    success_df = df_gaze[df_gaze['success'] == 1]\n",
    "\n",
    "    if not success_df.empty:\n",
    "        # Calculate the average direction of both eyes (0 and 1)\n",
    "        avg_y_gaze = (success_df['y_0'].mean() + success_df['y_1'].mean()) / 2\n",
    "        # Calculate how often the camera lost them (maybe they looked away entirely?)\n",
    "        tracking_rate = len(success_df) / len(df_gaze)\n",
    "    else:\n",
    "        avg_y_gaze = 0\n",
    "        tracking_rate = 0\n",
    "\n",
    "    return {\n",
    "        \"avg_gaze_downward\": avg_y_gaze, # Negative means looking up, Positive means looking down\n",
    "        \"eye_contact_rate\": tracking_rate # How much of the time they were 'visible'\n",
    "    }\n",
    "\n",
    "## 4. Load the covarep Audio\n",
    "def process_audio_covarep(covarep_path):\n",
    "    df = pd.read_csv(covarep_path, header=None, sep=\",\", engine=\"python\")\n",
    "    df = df[(df != 0).any(axis=1)]\n",
    "\n",
    "    features = {}\n",
    "    if df.empty:\n",
    "        for i in range(74):\n",
    "            features[f\"covarep_{i}_mean\"] = 0.0\n",
    "            features[f\"covarep_{i}_std\"] = 0.0\n",
    "        return features\n",
    "\n",
    "    for i in df.columns:\n",
    "        features[f\"covarep_{i}_mean\"] = df[i].mean()\n",
    "        features[f\"covarep_{i}_std\"] = df[i].std()\n",
    "\n",
    "    return features\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# AUDIO – FORMANT\n",
    "# ------------------------------------------------------------\n",
    "def process_formant(path):\n",
    "    # Correct separator = comma\n",
    "    df = pd.read_csv(path, header=None, sep=\",\", engine=\"python\")\n",
    "\n",
    "    # Ensure numeric (defensive programming)\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    df = df.dropna()\n",
    "\n",
    "    if df.empty:\n",
    "        return {\n",
    "            \"formant_F1_mean\": 0.0, \"formant_F1_std\": 0.0,\n",
    "            \"formant_F2_mean\": 0.0, \"formant_F2_std\": 0.0,\n",
    "            \"formant_F3_mean\": 0.0, \"formant_F3_std\": 0.0,\n",
    "            \"formant_F4_mean\": 0.0, \"formant_F4_std\": 0.0,\n",
    "            \"formant_F5_mean\": 0.0, \"formant_F5_std\": 0.0,\n",
    "        }\n",
    "\n",
    "    features = {}\n",
    "    for i, fname in enumerate([\"F1\", \"F2\", \"F3\", \"F4\", \"F5\"]):\n",
    "        features[f\"formant_{fname}_mean\"] = df[i].mean()\n",
    "        features[f\"formant_{fname}_std\"] = df[i].std()\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# HEAD POSE\n",
    "# ------------------------------------------------------------\n",
    "def process_pose(path):\n",
    "    # Explicit comma separator (this file IS CSV)\n",
    "    df = pd.read_csv(path, sep=\",\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Keep only successful frames\n",
    "    df = df[df[\"success\"] == 1]\n",
    "\n",
    "    # Force numeric conversion (defensive)\n",
    "    pose_cols = [\"Tx\", \"Ty\", \"Tz\", \"Rx\", \"Ry\", \"Rz\"]\n",
    "    df[pose_cols] = df[pose_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    if df.empty:\n",
    "        return {\n",
    "            \"pose_Tx_mean\": 0.0, \"pose_Ty_mean\": 0.0, \"pose_Tz_mean\": 0.0,\n",
    "            \"pose_Rx_mean\": 0.0, \"pose_Ry_mean\": 0.0, \"pose_Rz_mean\": 0.0,\n",
    "            \"pose_Rx_std\": 0.0,  \"pose_Ry_std\": 0.0,  \"pose_Rz_std\": 0.0,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"pose_Tx_mean\": df[\"Tx\"].mean(),\n",
    "        \"pose_Ty_mean\": df[\"Ty\"].mean(),\n",
    "        \"pose_Tz_mean\": df[\"Tz\"].mean(),\n",
    "        \"pose_Rx_mean\": df[\"Rx\"].mean(),\n",
    "        \"pose_Ry_mean\": df[\"Ry\"].mean(),\n",
    "        \"pose_Rz_mean\": df[\"Rz\"].mean(),\n",
    "        \"pose_Rx_std\": df[\"Rx\"].std(),\n",
    "        \"pose_Ry_std\": df[\"Ry\"].std(),\n",
    "        \"pose_Rz_std\": df[\"Rz\"].std(),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FACE GEOMETRY 2D\n",
    "# ------------------------------------------------------------\n",
    "def process_geom2d(path):\n",
    "    # Auto-detect delimiter (OpenFace files are inconsistent)\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Keep only valid frames\n",
    "    df = df[df[\"success\"] == 1]\n",
    "\n",
    "    # Select 2D landmark columns\n",
    "    geom_cols = [c for c in df.columns if c.startswith(\"x\") or c.startswith(\"y\")]\n",
    "\n",
    "    # Force numeric conversion column-by-column\n",
    "    for col in geom_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Drop invalid rows\n",
    "    df = df.dropna(subset=geom_cols)\n",
    "\n",
    "    if df.empty:\n",
    "        return {f\"geom2d_{c}\": 0.0 for c in geom_cols}\n",
    "\n",
    "    return (\n",
    "        df[geom_cols]\n",
    "        .astype(float)            # GUARANTEE numeric\n",
    "        .mean()\n",
    "        .add_prefix(\"geom2d_\")\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FACE GEOMETRY 3D\n",
    "# ------------------------------------------------------------\n",
    "def process_geom3d(path):\n",
    "    df = pd.read_csv(path, sep=\",\", engine=\"python\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    df = df[df[\"success\"] == 1]\n",
    "\n",
    "    geom_cols = [c for c in df.columns\n",
    "                 if c.startswith(\"X\") or c.startswith(\"Y\") or c.startswith(\"Z\")]\n",
    "\n",
    "    for col in geom_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=geom_cols)\n",
    "\n",
    "    if df.empty:\n",
    "        return {f\"geom3d_{c}\": 0.0 for c in geom_cols}\n",
    "\n",
    "    return (\n",
    "        df[geom_cols]\n",
    "        .mean()\n",
    "        .add_prefix(\"geom3d_\")\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# HOG (WITH PCA)\n",
    "# ------------------------------------------------------------\n",
    "def process_hog(hog_path, pose_path, n_components=20):\n",
    "    num_frames = len(pd.read_csv(pose_path))\n",
    "    data = np.fromfile(hog_path, dtype=np.float32)\n",
    "\n",
    "    if data.size == 0 or data.size % num_frames != 0:\n",
    "        return {f\"hog_pca_{i}_mean\":0.0 for i in range(n_components)}\n",
    "\n",
    "    hog_dim = data.size // num_frames\n",
    "    hog = data.reshape(num_frames, hog_dim)\n",
    "\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    hog_pca = pca.fit_transform(hog)\n",
    "\n",
    "    feats = {}\n",
    "    for i in range(n_components):\n",
    "        feats[f\"hog_pca_{i}_mean\"] = hog_pca[:,i].mean()\n",
    "        feats[f\"hog_pca_{i}_std\"] = hog_pca[:,i].std()\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c3793-c97b-46b0-bd47-fca1ea45a2c5",
   "metadata": {},
   "source": [
    "### Transcript Processing Sanity Check (Text Feature Validation)\n",
    "Runs the text preprocessing function on a single participant’s transcript to verify correct loading, speaker filtering, and sentence aggregation before applying the pipeline to the full dataset. This acts as a quick sanity check to confirm linguistic features are extracted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9dc1309a-7e48-43b4-9f45-203e924f4ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thank you mmm k i'm doing good thank you i'm from los angeles oh great i live in west los angeles the west side it's alright i xxx no i live alone so i love it i'm from here so i grew up here it's natural the weather um well the weather it's always good it's never it's never bad uh um there's always something to do it's rarely a dull moment the traffic the traffic is horrible well probably traffic is horrible in almost any major city but i hate the traffic not really i mean i have enough things going on here so if i travel it's usually somewhere that's within driving distance i studied uh business i did no i've i've been done for a few years so i haven't gone to school for a while one of these days i'll go back to graduate school but my dream job would be to just work for myself and making lots of money um i don't know i don't really have a dream job just something that i can i can work under my own terms and get paid decently and and be in a creative creative environment um i don't know i think it's just a matter of finding the right situation so right now eh it's been people are a little conservative about what they want so and how they want it so it's a little tougher than it seems <se> it should be but i don't know i think when the situation's right i don't think it's too difficult yep i work as an assistant <as> administrative assistant through a temp agency so it's just basically i get sent out to do uh to do desk jobs  um yes and no i mean i feel like i feel like i could do more but you know it works so  you know yeah i'm pretty close i mean they're around so um god really mad stupid people just basically doing just doing anything to annoy me um that's basically <ba> that's the big thing just just irritating stupid people <laughter> who just do things just to provoke provoke me for no reason at all um what's a good example just random people who who think it's funny to just sit there and get a rise out of me just it doesn't have to be one any specific situation it's just people who just think it's funny to i think they're just generally sadistic and they just think it's it's funny to to just prod you like it's like poking a stick at an angry dog knowing that they know how to they just wanna push your buttons for no reason so uh god i don't remember i with those situations i usually try not to remember <laughter> when when it happens i just i just let it go and just like the past is the past i just tend not to look back what do you mean okay god what do i do to relax i like to run i like to go to the gym listen to music i have a lot of musician friends so i just so i'm usually around musicians so anything of that sort i like i like everything from punk rock art to tattoos to just anything that's that's generally art and creative who's been a positive <pos> i think it's just friends in general who who seem to who seem to do well and i think they they're very inspiring and which is sometimes a a bit tough in in a town like los angeles where i think everybody's on the go trying to accomplish their own goals so they don't it's sometimes hard to find someone who's gonna be uplifting or try to be inspiring so i don't really have anybody in particular but people have kind of brought up things here and there you know just like hey instead of instead of doing this how about doing that or hey you're good at this why aren't you doing that or something like that just you know i'm always looking for new opportunities so they always point out things that maybe i might i could do differently instead of just being negative saying hey don't do that that's no that's why are you doing that it's a waste of time um i guess it depends on who you talk to i'm i'm okay at it i could probably work at that um i don't know i mean i can i i think i can do better at it well back to the point about people provoking me so i mean i try not to blow steam and let them get under my skin so i just usually try not to let it get to me but i don't know i guess with temper i guess it depends on how i handle my stress lately i've been better at it just because i think my situation's been a little bit better no no no um nothing major i mean maybe when i was younger i could've traveled more or i could've worked towards something that paid better job-wise but i mean those are all things that i think at any point in life you always think like well i coulda shoulda would've done but you know but then again i think that's natural for anybody i mean i could i could've become a banker or i could've become a lawyer or a doctor and made about ten times the money that i make now but but then again i don't like the law and i don't like medicine so so i'm it wasn't meant to be so yeah so um i think i think my in my life i knew that i there's a lot of things i have <ha> there's more dislikes <laughter> than likes so i kinda narrowed it down to what am i good at and what am i not good at and what am i gonna work well or who who am i gonna work well with and who will i not work well with so i kind of i kinda sorted out and then the list kind of mmm kind of answered itself so no it i don't think it was hard but it was just but i think it was a real reality check and i think it it's kind of a good thing 'cause sometimes trying to conform to doing things that doesn't really fit you doesn't make sense it's like trying to shove a a round peg into a square a square hole and it's like it just no matter how you try to shove it in it's not gonna go in so sometimes it's just might as well go down a path that seems to work better for you memorable experiences um hmm i think it's just i don't really have like any one in particular i think i think every day is almost like a memorable experience whether it's positive or negative i think it's just everything from getting it just just the fact that i got a job and i can i can take care of myself is already almost like a memorable experience maybe for the average person it it sounds stupid but you know i think just the fact that i'm able i'm able to get this far in life without like completely self-destructing is already an accomplishment so i feel like i've gotten somewhere maybe not to not maybe not to the point to the average person they might think like well why aren't you a millionaire but you know what not everybody can be a millionaire yeah um when was the last time i really felt happy um i don't know i'm not really someone who's like i don't have any real high highs or low lows i mean i haven't hit any lows so i mean i i don't know i'm usually pretty i feel like i'm a level person and i'm pretty happy <ha> every day i feel like if i accomplish one thing then i'm pretty happy so i guess to answer the question probably mmm probably yesterday i just feel like if i if i'm able to accomplish something then then hey i'm happy i don't know i don't really have a best friend but i mean i well to answer the question generally friends i think they would say that i'm very <ver> pretty outgoing i'm pretty determined i'm trying you know i try to you know i try to interact as much as possible i'm always out networking as much as possible so i don't know i guess that's basically what i think they would describe me as okay no problem alright bye\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_words(BASE_PATH / \"DAIC_WOZ_subset/extracted/301_P/301_TRANSCRIPT.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a34f9c-301d-4343-b346-35176b39014f",
   "metadata": {},
   "source": [
    "### Facial Action Unit Feature Extraction Test (Expression Signals)\n",
    "Executes the facial Action Unit preprocessing on a single participant file to confirm correct parsing and computation of average facial muscle activation intensities, ensuring expression-based behavioral features are properly generated before full-scale processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed6dcb70-b5d9-4064-bfe0-9396eb207ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AU01_r': 0.13259438837425672,\n",
       " 'AU02_r': 0.06735483224788641,\n",
       " 'AU04_r': 0.09583429792484124,\n",
       " 'AU05_r': 0.029404565834715426,\n",
       " 'AU06_r': 0.018630478500060676,\n",
       " 'AU09_r': 0.08655279337405444,\n",
       " 'AU10_r': 0.03926264774887747,\n",
       " 'AU12_r': 0.04907692556935399,\n",
       " 'AU14_r': 0.10778485000606772,\n",
       " 'AU15_r': 0.012307463249868532,\n",
       " 'AU17_r': 0.0198868424416488,\n",
       " 'AU20_r': 0.08869211103919744,\n",
       " 'AU25_r': 0.5712136212531855,\n",
       " 'AU26_r': 0.0013668396504995751}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_face_aus(BASE_PATH / \"DAIC_WOZ_subset/extracted/301_P/301_CLNF_AUs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b73e69-11a0-4485-93a7-d3f764a0c460",
   "metadata": {},
   "source": [
    "### Eye Gaze Feature Extraction Test (Behavioral Attention Signals)\n",
    "Runs the eye gaze preprocessing on a single participant file to verify correct filtering of valid frames and computation of gaze direction and eye-contact rate, ensuring reliable behavioral attention features before applying the pipeline to all participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c0e5109-c020-427b-941c-04c649bb14f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_gaze_downward': np.float64(0.2383617907058136),\n",
       " 'eye_contact_rate': 0.9911815865054002}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_eyes_carefully(BASE_PATH / \"DAIC_WOZ_subset/extracted/301_P/301_CLNF_gaze.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f48fc7-2b8e-405a-9e55-f2e65aa49218",
   "metadata": {},
   "source": [
    "### Audio Feature Extraction Test (COVAREP Acoustic Descriptors)\n",
    "Processes a single participant’s COVAREP audio file to compute statistical summaries (mean and standard deviation) of low-level acoustic features, validating that vocal characteristics are correctly extracted before large-scale multimodal integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6cf82519-bd9a-4918-9c12-eabb3a4ad092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covarep = process_audio_covarep(BASE_PATH / \"DAIC_WOZ_subset/extracted/301_P/301_COVAREP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0321c4a8-81ac-4fa4-8d2a-66cd61a6198e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covarep_0_mean': np.float64(127.25741300416318),\n",
       " 'covarep_0_std': np.float64(41.00267268920412),\n",
       " 'covarep_1_mean': np.float64(0.3928922550340458),\n",
       " 'covarep_1_std': np.float64(0.4883961774485749),\n",
       " 'covarep_2_mean': np.float64(0.06589149353629005),\n",
       " 'covarep_2_std': np.float64(0.056274400537690755),\n",
       " 'covarep_3_mean': np.float64(0.22602470152321305),\n",
       " 'covarep_3_std': np.float64(0.19343784273331557),\n",
       " 'covarep_4_mean': np.float64(0.08172966152151379),\n",
       " 'covarep_4_std': np.float64(4.95747557224423),\n",
       " 'covarep_5_mean': np.float64(0.15661044655490175),\n",
       " 'covarep_5_std': np.float64(0.16272447720893676),\n",
       " 'covarep_6_mean': np.float64(0.09910476905048003),\n",
       " 'covarep_6_std': np.float64(0.03222231641927864),\n",
       " 'covarep_7_mean': np.float64(-0.3527951451730207),\n",
       " 'covarep_7_std': np.float64(0.0734492510601278),\n",
       " 'covarep_8_mean': np.float64(1.3987305773829033),\n",
       " 'covarep_8_std': np.float64(0.5078544840944021),\n",
       " 'covarep_9_mean': np.float64(0.6033904647465075),\n",
       " 'covarep_9_std': np.float64(0.12989953984831923),\n",
       " 'covarep_10_mean': np.float64(0.0),\n",
       " 'covarep_10_std': np.float64(0.0),\n",
       " 'covarep_11_mean': np.float64(-9.252646633652551),\n",
       " 'covarep_11_std': np.float64(0.8601041074960902),\n",
       " 'covarep_12_mean': np.float64(1.9292111308593514),\n",
       " 'covarep_12_std': np.float64(0.7139816806428865),\n",
       " 'covarep_13_mean': np.float64(0.26628527611990677),\n",
       " 'covarep_13_std': np.float64(0.4189015067222493),\n",
       " 'covarep_14_mean': np.float64(0.37244217001129765),\n",
       " 'covarep_14_std': np.float64(0.28439338717805174),\n",
       " 'covarep_15_mean': np.float64(0.054613443163703895),\n",
       " 'covarep_15_std': np.float64(0.214916383564498),\n",
       " 'covarep_16_mean': np.float64(0.09704034942745147),\n",
       " 'covarep_16_std': np.float64(0.2256343584723955),\n",
       " 'covarep_17_mean': np.float64(-0.017984399798626514),\n",
       " 'covarep_17_std': np.float64(0.18247015236947464),\n",
       " 'covarep_18_mean': np.float64(0.1564302848611708),\n",
       " 'covarep_18_std': np.float64(0.1656726434855307),\n",
       " 'covarep_19_mean': np.float64(-0.0030329409260653733),\n",
       " 'covarep_19_std': np.float64(0.14663228046873103),\n",
       " 'covarep_20_mean': np.float64(0.06321111299664543),\n",
       " 'covarep_20_std': np.float64(0.125489757734406),\n",
       " 'covarep_21_mean': np.float64(-0.0522056616393879),\n",
       " 'covarep_21_std': np.float64(0.12226162456557399),\n",
       " 'covarep_22_mean': np.float64(0.024864169688161044),\n",
       " 'covarep_22_std': np.float64(0.12978015042413912),\n",
       " 'covarep_23_mean': np.float64(-0.046375818528707716),\n",
       " 'covarep_23_std': np.float64(0.1281959503286362),\n",
       " 'covarep_24_mean': np.float64(0.06507368591896127),\n",
       " 'covarep_24_std': np.float64(0.10646370675690521),\n",
       " 'covarep_25_mean': np.float64(-0.037268220912471325),\n",
       " 'covarep_25_std': np.float64(0.1089124881976499),\n",
       " 'covarep_26_mean': np.float64(0.022265170466270013),\n",
       " 'covarep_26_std': np.float64(0.08985245104392059),\n",
       " 'covarep_27_mean': np.float64(0.006072710185259196),\n",
       " 'covarep_27_std': np.float64(0.09107375881195451),\n",
       " 'covarep_28_mean': np.float64(0.02432086439365437),\n",
       " 'covarep_28_std': np.float64(0.08706631934544076),\n",
       " 'covarep_29_mean': np.float64(-0.015645039802568303),\n",
       " 'covarep_29_std': np.float64(0.07502304535168051),\n",
       " 'covarep_30_mean': np.float64(0.03276210872076527),\n",
       " 'covarep_30_std': np.float64(0.07547425691092471),\n",
       " 'covarep_31_mean': np.float64(0.02031076013867385),\n",
       " 'covarep_31_std': np.float64(0.0702445944719592),\n",
       " 'covarep_32_mean': np.float64(0.011937070026847152),\n",
       " 'covarep_32_std': np.float64(0.06529991349934149),\n",
       " 'covarep_33_mean': np.float64(-0.03538406282646349),\n",
       " 'covarep_33_std': np.float64(0.06368680548469531),\n",
       " 'covarep_34_mean': np.float64(0.022042845133760574),\n",
       " 'covarep_34_std': np.float64(0.06112812116177893),\n",
       " 'covarep_35_mean': np.float64(0.0013698236138864413),\n",
       " 'covarep_35_std': np.float64(0.06406353967767214),\n",
       " 'covarep_36_mean': np.float64(0.0),\n",
       " 'covarep_36_std': np.float64(0.0),\n",
       " 'covarep_37_mean': np.float64(0.0),\n",
       " 'covarep_37_std': np.float64(0.0),\n",
       " 'covarep_38_mean': np.float64(0.0),\n",
       " 'covarep_38_std': np.float64(0.0),\n",
       " 'covarep_39_mean': np.float64(0.0),\n",
       " 'covarep_39_std': np.float64(0.0),\n",
       " 'covarep_40_mean': np.float64(0.0013900151157178785),\n",
       " 'covarep_40_std': np.float64(0.007289356682084226),\n",
       " 'covarep_41_mean': np.float64(0.010520893353650463),\n",
       " 'covarep_41_std': np.float64(0.04268131531634017),\n",
       " 'covarep_42_mean': np.float64(0.013448156268980884),\n",
       " 'covarep_42_std': np.float64(0.08050501660120522),\n",
       " 'covarep_43_mean': np.float64(-0.03296527652533348),\n",
       " 'covarep_43_std': np.float64(0.15913991596384067),\n",
       " 'covarep_44_mean': np.float64(-0.1650549524204185),\n",
       " 'covarep_44_std': np.float64(0.3517035627559385),\n",
       " 'covarep_45_mean': np.float64(-0.4822983360575525),\n",
       " 'covarep_45_std': np.float64(0.8173159863147617),\n",
       " 'covarep_46_mean': np.float64(-0.3900511399637816),\n",
       " 'covarep_46_std': np.float64(1.4096234717611673),\n",
       " 'covarep_47_mean': np.float64(0.26509329535448906),\n",
       " 'covarep_47_std': np.float64(1.6353308383702123),\n",
       " 'covarep_48_mean': np.float64(0.6925740006884777),\n",
       " 'covarep_48_std': np.float64(1.8433841090754481),\n",
       " 'covarep_49_mean': np.float64(0.9853150624813386),\n",
       " 'covarep_49_std': np.float64(1.8409017921869735),\n",
       " 'covarep_50_mean': np.float64(1.1594655753276772),\n",
       " 'covarep_50_std': np.float64(1.781309789318971),\n",
       " 'covarep_51_mean': np.float64(1.2211596909680298),\n",
       " 'covarep_51_std': np.float64(1.746501496394016),\n",
       " 'covarep_52_mean': np.float64(1.1894432791787253),\n",
       " 'covarep_52_std': np.float64(1.7656161859998567),\n",
       " 'covarep_53_mean': np.float64(1.1145686578028864),\n",
       " 'covarep_53_std': np.float64(1.8226462970385826),\n",
       " 'covarep_54_mean': np.float64(1.026685353314326),\n",
       " 'covarep_54_std': np.float64(1.882491287623063),\n",
       " 'covarep_55_mean': np.float64(0.9464001265844227),\n",
       " 'covarep_55_std': np.float64(2.1669177307106637),\n",
       " 'covarep_56_mean': np.float64(1.203946571800908),\n",
       " 'covarep_56_std': np.float64(2.0189181732957597),\n",
       " 'covarep_57_mean': np.float64(1.4068591584022139),\n",
       " 'covarep_57_std': np.float64(1.8233533955950227),\n",
       " 'covarep_58_mean': np.float64(0.9937995192968962),\n",
       " 'covarep_58_std': np.float64(1.8423311907617903),\n",
       " 'covarep_59_mean': np.float64(0.8037329724987196),\n",
       " 'covarep_59_std': np.float64(1.982806887244852),\n",
       " 'covarep_60_mean': np.float64(0.3223307117969632),\n",
       " 'covarep_60_std': np.float64(2.1695555169591025),\n",
       " 'covarep_61_mean': np.float64(-0.25651962893545316),\n",
       " 'covarep_61_std': np.float64(0.4371576850295762),\n",
       " 'covarep_62_mean': np.float64(-0.9283260474092415),\n",
       " 'covarep_62_std': np.float64(0.38323433747044305),\n",
       " 'covarep_63_mean': np.float64(-0.6573714794252752),\n",
       " 'covarep_63_std': np.float64(0.21579993140153908),\n",
       " 'covarep_64_mean': np.float64(-0.6682831121958028),\n",
       " 'covarep_64_std': np.float64(0.17968556448460238),\n",
       " 'covarep_65_mean': np.float64(-0.617557134954909),\n",
       " 'covarep_65_std': np.float64(0.14766384496789128),\n",
       " 'covarep_66_mean': np.float64(-0.5710802960103898),\n",
       " 'covarep_66_std': np.float64(0.12709966312674448),\n",
       " 'covarep_67_mean': np.float64(-0.5358264522448385),\n",
       " 'covarep_67_std': np.float64(0.10428660534912154),\n",
       " 'covarep_68_mean': np.float64(-0.4651191031895035),\n",
       " 'covarep_68_std': np.float64(0.09930485235599358),\n",
       " 'covarep_69_mean': np.float64(-0.4354096764108073),\n",
       " 'covarep_69_std': np.float64(0.08596070553784857),\n",
       " 'covarep_70_mean': np.float64(-0.37752939605602687),\n",
       " 'covarep_70_std': np.float64(0.0923355282606287),\n",
       " 'covarep_71_mean': np.float64(-0.3494193131026108),\n",
       " 'covarep_71_std': np.float64(0.09000428535558765),\n",
       " 'covarep_72_mean': np.float64(-0.29751047153672944),\n",
       " 'covarep_72_std': np.float64(0.09926610380216638),\n",
       " 'covarep_73_mean': np.float64(-0.2489722459760937),\n",
       " 'covarep_73_std': np.float64(0.1118503736091143)}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covarep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253213b-f8cb-4b42-87a9-b6d05f1c361a",
   "metadata": {},
   "source": [
    "###  Participant-Level Multimodal Feature Aggregation (Master Row Builder)\n",
    "Combines all modality-specific preprocessing functions into a single pipeline that extracts and merges linguistic, facial, gaze, audio, pose, and geometric features for one participant, producing a unified feature vector (one row per participant) ready for dataset construction and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cb45df4a-6787-4869-a655-b9bdca244078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASTER FUNCTION (ONE PARTICIPANT)\n",
    "# ------------------------------------------------------------\n",
    "def build_master_row(pid, base_path):\n",
    "    p = f\"{base_path}/{pid}_P\"\n",
    "    row = {\"participant_id\": pid}\n",
    "\n",
    "    row[\"text\"] = process_words(f\"{p}/{pid}_TRANSCRIPT.csv\")\n",
    "    row.update(process_face_aus(f\"{p}/{pid}_CLNF_AUs.txt\"))\n",
    "    row.update(process_eyes_carefully(f\"{p}/{pid}_CLNF_gaze.txt\"))\n",
    "    row.update(process_audio_covarep(f\"{p}/{pid}_COVAREP.csv\"))\n",
    "    row.update(process_formant(f\"{p}/{pid}_FORMANT.csv\"))\n",
    "    row.update(process_pose(f\"{p}/{pid}_CLNF_pose.txt\"))\n",
    "    row.update(process_geom2d(f\"{p}/{pid}_CLNF_features.txt\"))\n",
    "    row.update(process_geom3d(f\"{p}/{pid}_CLNF_features3D.txt\"))\n",
    "    row.update(process_hog(\n",
    "        f\"{p}/{pid}_CLNF_hog.bin\",\n",
    "        f\"{p}/{pid}_CLNF_pose.txt\"\n",
    "    ))\n",
    "\n",
    "    return pd.DataFrame([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5239dcc-487d-4156-b596-5ceb7eea7bda",
   "metadata": {},
   "source": [
    "### Full Dataset Construction and Master Feature Matrix Generation\n",
    "Iterates through all participant folders, applies the multimodal feature aggregation pipeline to each subject, and concatenates the resulting rows into a single participant-level master dataset. The final unified feature matrix is then saved as a CSV file for downstream modeling and experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "052222e6-7b11-4a58-b2ce-34f412db1d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301_P\n",
      "302_P\n",
      "303_P\n",
      "304_P\n",
      "305_P\n",
      "306_P\n",
      "307_P\n",
      "308_P\n",
      "309_P\n",
      "310_P\n",
      "312_P\n",
      "313_P\n",
      "314_P\n",
      "315_P\n",
      "316_P\n",
      "317_P\n",
      "318_P\n",
      "319_P\n",
      "320_P\n",
      "321_P\n",
      "322_P\n",
      "323_P\n",
      "324_P\n",
      "325_P\n",
      "326_P\n",
      "327_P\n",
      "328_P\n",
      "329_P\n",
      "330_P\n",
      "331_P\n",
      "332_P\n",
      "333_P\n",
      "334_P\n",
      "335_P\n",
      "336_P\n",
      "337_P\n",
      "338_P\n",
      "339_P\n",
      "340_P\n",
      "341_P\n",
      "343_P\n",
      "344_P\n",
      "345_P\n",
      "346_P\n",
      "347_P\n",
      "348_P\n",
      "349_P\n",
      "350_P\n",
      "351_P\n",
      "352_P\n",
      "353_P\n",
      "354_P\n",
      "355_P\n",
      "356_P\n",
      "357_P\n",
      "358_P\n",
      "359_P\n",
      "360_P\n",
      "361_P\n",
      "362_P\n",
      "363_P\n",
      "364_P\n",
      "365_P\n",
      "366_P\n",
      "367_P\n",
      "368_P\n",
      "369_P\n",
      "370_P\n",
      "371_P\n",
      "372_P\n",
      "373_P\n",
      "374_P\n",
      "375_P\n",
      "376_P\n",
      "377_P\n",
      "378_P\n",
      "379_P\n",
      "380_P\n",
      "381_P\n",
      "382_P\n",
      "383_P\n",
      "384_P\n",
      "385_P\n",
      "386_P\n",
      "387_P\n",
      "388_P\n",
      "389_P\n",
      "390_P\n",
      "391_P\n",
      "392_P\n",
      "393_P\n",
      "395_P\n",
      "396_P\n",
      "397_P\n",
      "399_P\n",
      "400_P\n",
      "401_P\n",
      "402_P\n",
      "403_P\n",
      "404_P\n",
      "405_P\n",
      "406_P\n",
      "407_P\n",
      "408_P\n",
      "409_P\n",
      "410_P\n",
      "411_P\n",
      "412_P\n",
      "413_P\n",
      "414_P\n",
      "415_P\n",
      "416_P\n",
      "417_P\n",
      "418_P\n",
      "419_P\n",
      "420_P\n",
      "421_P\n",
      "422_P\n",
      "423_P\n",
      "424_P\n",
      "425_P\n",
      "426_P\n",
      "427_P\n",
      "428_P\n",
      "429_P\n",
      "430_P\n",
      "431_P\n",
      "432_P\n",
      "433_P\n",
      "434_P\n",
      "435_P\n",
      "436_P\n",
      "437_P\n",
      "438_P\n",
      "439_P\n",
      "440_P\n",
      "441_P\n",
      "442_P\n",
      "443_P\n",
      "444_P\n",
      "445_P\n",
      "446_P\n",
      "447_P\n",
      "448_P\n",
      "449_P\n",
      "450_P\n",
      "451_P\n",
      "452_P\n",
      "453_P\n",
      "454_P\n",
      "455_P\n",
      "456_P\n",
      "457_P\n",
      "458_P\n",
      "459_P\n",
      "461_P\n",
      "462_P\n",
      "463_P\n",
      "464_P\n",
      "465_P\n",
      "466_P\n",
      "467_P\n",
      "468_P\n",
      "469_P\n",
      "470_P\n",
      "471_P\n",
      "472_P\n",
      "473_P\n",
      "474_P\n",
      "475_P\n",
      "476_P\n",
      "477_P\n",
      "478_P\n",
      "479_P\n",
      "480_P\n",
      "481_P\n",
      "482_P\n",
      "483_P\n",
      "484_P\n",
      "485_P\n",
      "486_P\n",
      "487_P\n",
      "488_P\n",
      "489_P\n",
      "490_P\n",
      "491_P\n",
      "492_P\n",
      "File saved successfully to: D:/portfolio/IU/Thesis_Final/Coding/master_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# BASE_PATH = Path('DAIC_WOZ_subset')\n",
    "ITERATE_PATH = BASE_PATH / \"DAIC_WOZ_subset/extracted\"\n",
    "\n",
    "# 1. Initialize an empty list (NOT an empty DataFrame)\n",
    "rows_list = []\n",
    "\n",
    "# i = 1 \n",
    "for item in ITERATE_PATH.iterdir():\n",
    "    if item.is_dir():\n",
    "        df_master_single = build_master_row(item.name.replace(\"_P\", \"\"), ITERATE_PATH)\n",
    "        \n",
    "        # Append the small DataFrame to your list\n",
    "        rows_list.append(df_master_single)\n",
    "        print(item.name)\n",
    "        # i += 1\n",
    "        # if i > 10:\n",
    "        #     break\n",
    "# 3. Concatenate everything ONCE at the end\n",
    "df_master = pd.concat(rows_list, ignore_index=True)\n",
    "\n",
    "# Save the final concatenated DataFrame\n",
    "output_path = \"D:/portfolio/IU/Thesis_Final/Coding/master_dataset.csv\"\n",
    "\n",
    "df_master.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"File saved successfully to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
